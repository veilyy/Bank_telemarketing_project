# src/train.py
import pandas as pd
import joblib
import json
import os
from datetime import datetime
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier

# –í–∞—à–∏ –º–æ–¥—É–ª–∏
from src.preprocessing import Preprocessor
from src.model import ModelTrainer

def main():
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    df = pd.read_csv('data/raw/bank-additional-full.csv', sep=';')
    
    # 2. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ X –∏ y
    df['y'] = df['y'].map({'yes': 1, 'no': 0})
    X = df.drop('y', axis=1)
    y = df['y']
    
    print(f"–í—Å–µ–≥–æ –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} —Å—Ç—Ä–æ–∫, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {y.sum()} 'yes' ({y.mean():.2%})")
    
    # 3. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ–Ω—é –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
    print("\n" + "="*50)
    print("ü§ñ –í–´–ë–ï–†–ò–¢–ï –ú–û–î–ï–õ–¨ –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø:")
    print("="*50)
    print("1. XGBoost (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã) - –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø")
    print("2. Random Forest")
    print("3. Logistic Regression")
    print("4. XGBoost (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)")
    print("="*50)
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—ã–±–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    choice = input()
    
    # 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±–æ—Ä–∞
    if choice == '1':  # XGBoost –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
        model_class = XGBClassifier
        model_name = "xgboost"
        model_params = {
            'n_estimators': 226,
            'max_depth': 7,
            'learning_rate': 0.01921106148185441,
            'subsample': 0.7915715432646518,
            'colsample_bytree': 0.9251010796404475,
            'min_child_weight': 5,
            'gamma': 0.10900791869271426,
            'reg_alpha': 0.01961633761408354,
            'reg_lambda': 0.006040374222520123,
            'scale_pos_weight': 5.787866022971578,
            'random_state': 42
        }
        optimal_threshold = 0.5996857774477184
        
    elif choice == '2':  # Random Forest
        model_class = RandomForestClassifier
        model_name = "random_forest"
        model_params = {
            'n_estimators': 200,
            'max_depth': 15,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'class_weight': 'balanced',
            'random_state': 42,
            'n_jobs': -1
        }
        optimal_threshold = 0.5

        
    elif choice == '3':  # Logistic Regression
        model_class = LogisticRegression
        model_name = "logistic_regression"
        model_params = {
            'C': 0.1,
            'class_weight': 'balanced',
            'max_iter': 1000,
            'random_state': 42,
            'solver': 'liblinear'
        }
        optimal_threshold = 0.67

    else:  # XGBoost –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 
        model_class = XGBClassifier
        model_name = "xgboost_default"
        model_params = {
            'n_estimators': 100,
            'max_depth': 5,
            'learning_rate': 0.1,
            'scale_pos_weight': len(y[y==0]) / len(y[y==1]),
            'random_state': 42
        }
        optimal_threshold = 0.6
    
    print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model_name}")
    
    # 5. –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–µ –ø–∞–ø–∫–∏
    os.makedirs('models/preprocessor', exist_ok=True)   # –û–±—â–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    os.makedirs('models/metrics', exist_ok=True)        # –û–±—â–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –º–µ—Ç—Ä–∏–∫
    
    # 6. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    pipline = 
    
    # 7. –û–±—É—á–µ–Ω–∏–µ
    trainer.fit(X, y, test_size=0.2, threshold=optimal_threshold)
    
    # 8. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –≤ –æ–±—â—É—é –ø–∞–ø–∫—É preprocessor
    trainer.preprocessor.save('models/preprocessor/')
    print("‚úÖ –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ models/preprocessor/")
    
    # 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ –∫–æ—Ä–µ–Ω—å models
    model_filename = f'{model_name}_model.joblib'
    trainer.save(f'models/{model_filename}')
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ models/{model_filename}")
    
    # 10. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ –æ–±—â—É—é –ø–∞–ø–∫—É metrics
    metrics = trainer.metrics.copy()
    metrics['model_name'] = model_name
    metrics['model_class'] = model_class.__name__
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    with open('models/metrics/latest_metrics.json', 'w') as f:
        json.dump(metrics, f, indent=4)
    
        print("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ models/metrics/")
    
    # 11. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    params_to_save = {
        'model_name': model_name,
        'model_params': model_params,
        'threshold': optimal_threshold,
    }
    
    with open('models/metrics/model_params.json', 'w') as f:
        json.dump(params_to_save, f, indent=4)
    print("‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ models/metrics/model_params.json")
    
    # 12. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

    feature_importance = trainer.get_feature_importance()
    print(f"–¢–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {model_name}:")
    print(feature_importance.head().to_string())

    feature_importance.to_csv(
        f'models/metrics/feature_importance_{model_name}.csv', 
        index=False)

    # 13. –ò—Ç–æ–≥
    print("\n" + "="*50)
    print(f"ü§ñ –ú–æ–¥–µ–ª—å: {model_name}")
    print(f"üìä F1-score: {trainer.metrics['f1']:.4f}")
    print(f"üìà ROC-AUC:  {trainer.metrics['roc_auc']:.4f}")
    print(f"üéØ Threshold: {optimal_threshold:.4f}")
    print("="*50)